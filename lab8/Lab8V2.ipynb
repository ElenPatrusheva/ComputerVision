{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QcOXuSo0LyM4"
   },
   "source": [
    "## Lab 8. YOLO object detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9XoRkmPFzqCW"
   },
   "source": [
    "## Import libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QuSphY5uzsuP"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-36f07bc1d240>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminidom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# all plots will be set directly below the code cell that produced it\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "# set inline plots size\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 10) # (w, h)\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import zipfile\n",
    "import tarfile\n",
    "from collections import namedtuple\n",
    "from google.colab.patches import cv2_imshow\n",
    "from xml.dom import minidom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdOMlcJ2Lv2K"
   },
   "source": [
    "### Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_Pa3DOBlECR"
   },
   "outputs": [],
   "source": [
    "!wget \"https://miro.medium.com/max/872/1*EYFejGUjvjPcc4PZTwoufw.jpeg\" -O traffic.jpeg\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgdEBbFF26zv"
   },
   "source": [
    "### Download YOLO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZwUpjRKX1I_U"
   },
   "outputs": [],
   "source": [
    "!wget https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/opencv-yolo/yolo-object-detection.zip?__s=1essnpgyhz7jwwcpjszi -O yolo-object-detection.zip\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAbzya7rOBav"
   },
   "source": [
    "### Download Pascal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOCiAm5V3AZ0"
   },
   "outputs": [],
   "source": [
    "#unable to get it online, \n",
    "#!wget http://mrg.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar -O pascal.tar\n",
    "#http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar is alternative source, download speed so low, so I will share the required images separately\n",
    "#!wget http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar -O pascal.tar\n",
    "!wget 'https://drive.google.com/file/d/1Xcq9FH2FdgZH1EQLK0kaQxntqH5bPWny/view?usp=sharing' -O '2012_000160.jpg'\n",
    "!wget 'https://drive.google.com/file/d/10Lb2rfVtROB95xeS4vyGVMDaJhB3sgar/view?usp=sharing' -O '2012_000160.xml'\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hPS6i3Be_kIu"
   },
   "source": [
    "### Extract model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BTDyhbk21ek2"
   },
   "outputs": [],
   "source": [
    "# UNZIP YOLO\n",
    "with zipfile.ZipFile('yolo-object-detection.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('yolo_data')\n",
    "    \n",
    "\n",
    "#Not used    \n",
    "# UNZIP PascalVOC\n",
    "#if not os.path.exists('pascal'):\n",
    "#  os.makedirs('pascal')\n",
    "#tar = tarfile.open('/content/pascal.tar')\n",
    "#tar.extractall(path = 'pascal')\n",
    "#tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O-DIQjt7-kH6"
   },
   "source": [
    "## IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 132
    },
    "colab_type": "code",
    "id": "5lX_3SJWAVEm",
    "outputId": "e86f3ed5-2091-4373-a923-6850ff0def67"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-2b243f2b612b>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    left_x = < your code here >\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def IoU(box1, box2):\n",
    "  \"\"\"\n",
    "  box1: [x1,y1,x2,y2] coordinates of the ground truth box\n",
    "  box2: [x1,y1,x2,y2] coordinates of the predicted box\n",
    "  return: IoU between two boxes if they are overlapping, 0 otherwise\n",
    "  \n",
    "  with\n",
    "  x1, y1:  coordinates of the upper left corner\n",
    "  x2, y2: coordinates of the lower right corner\n",
    "  \"\"\"\n",
    "  # determine coordinates of the intersection\n",
    "  left_x = < your code here >\n",
    "  bottom_y = < your code here >\n",
    "  right_x = < your code here >\n",
    "  top_y = < your code here >\n",
    "  \n",
    "  # compute intersection area\n",
    "  interArea = < your code here >\n",
    "\n",
    "  # compute the area of both the prediction and ground-truth\n",
    "  ground_area = < your code here >\n",
    "  predicted_area = < your code here >\n",
    "\n",
    "  # compute the IoU\n",
    "  return < your code here >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WkSIQYcuKQDX"
   },
   "source": [
    "## YOLO with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 132
    },
    "colab_type": "code",
    "id": "Iru7yRh4KSPr",
    "outputId": "3d1c4330-e63e-4aed-bde0-3d06784a0bc5"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-2b5390c07e3c>\"\u001b[0;36m, line \u001b[0;32m105\u001b[0m\n\u001b[0;31m    < your code here >\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_YOLO = 'yolo_data/yolo-object-detection/yolo-coco'\n",
    "CONFIDENCE = 0.5\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "class YOLO:\n",
    "  def __init__(self, PATH_TO_YOLO, CONFIDENCE, THRESHOLD):\n",
    "    self.CONFIDENCE = CONFIDENCE\n",
    "    self.THRESHOLD = THRESHOLD\n",
    "    # load the COCO class labels our YOLO model was trained on\n",
    "    labelsPath = os.path.sep.join([PATH_TO_YOLO, 'coco.names'])\n",
    "    self.LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "    # initialize a list of colors to represent each possible class label\n",
    "    np.random.seed(42)\n",
    "    self.COLORS = np.random.randint(0, 255, size=(len(self.LABELS), 3),\tdtype=\"uint8\")\n",
    "\n",
    "    # derive the paths to the YOLO weights and model configuration\n",
    "    weightsPath = os.path.sep.join([PATH_TO_YOLO, \"yolov3.weights\"])\n",
    "    configPath = os.path.sep.join([PATH_TO_YOLO, \"yolov3.cfg\"])\n",
    "\n",
    "    # load YOLO object detector trained on COCO dataset (80 classes)\n",
    "    net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "    \n",
    "    # determine only the *output* layer names that we need from YOLO\n",
    "    ln = net.getLayerNames()\n",
    "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    \n",
    "    self.net = net\n",
    "    self.ln = ln\n",
    "    \n",
    "  def forward(self, image):\n",
    "    \"\"\"\n",
    "    feed an image to YOLO network, filter weak boxes, \n",
    "    return boxes, confidence, class labels\n",
    "    \"\"\"\n",
    "    (H, W) = image.shape[:2]\n",
    "    # construct a blob from the input image and then perform a forward\n",
    "    # pass of the YOLO object detector, giving us our bounding boxes and\n",
    "    # associated probabilities\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    self.net.setInput(blob)\n",
    "    start = time.time()\n",
    "    layerOutputs = self.net.forward(self.ln)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"[INFO] YOLO took {:.6f} seconds\".format(end - start))\n",
    "    \n",
    "    # initialize our lists of detected bounding boxes, confidences, and\n",
    "    # class IDs, respectively\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "\n",
    "    # loop over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "      # loop over each of the detections\n",
    "      for detection in output:\n",
    "        # extract the class ID and confidence\n",
    "        scores = detection[5:]\n",
    "        classID = np.argmax(scores)\n",
    "        confidence = scores[classID]\n",
    "\n",
    "        # filter out weak predictions \n",
    "        if confidence > self.CONFIDENCE:\n",
    "          # scale the bounding box coordinates back relative to the\n",
    "          # size of the image, keeping in mind that YOLO actually\n",
    "          # returns the center (x, y)-coordinates of the bounding\n",
    "          # box followed by the boxes' width and height\n",
    "          box = detection[0:4] * np.array([W, H, W, H])\n",
    "          (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "          # use the center (x, y)-coordinates to derive the top and\n",
    "          # and left corner of the bounding box\n",
    "          x = int(centerX - (width / 2))\n",
    "          y = int(centerY - (height / 2))\n",
    "\n",
    "          # update our list of bounding box coordinates, confidences,\n",
    "          # and class IDs\n",
    "          boxes.append([x, y, int(width), int(height)])\n",
    "          confidences.append(float(confidence))\n",
    "          classIDs.append(classID)\n",
    "          \n",
    "    self.boxes = np.array(boxes)\n",
    "    self.confidences = np.array(confidences)\n",
    "    self.classIDs = np.array(classIDs)\n",
    "    \n",
    "  def non_max_supression(self):\n",
    "    \"\"\"\n",
    "    perform non-maximum supression over boxes\n",
    "    \"\"\"\n",
    "    # can be done using cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE, THRESHOLD)\n",
    "    idxs = np.argsort(-self.confidences)\n",
    "    confidences = self.confidences[idxs]\n",
    "    boxes = self.boxes[idxs]\n",
    "    classIDs = self.classIDs[idxs]\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "      x,y,w,h = boxes[i][0],  boxes[i][1], boxes[i][2], boxes[i][3]\n",
    "      box1 = [x, y, x+w, y+h]\n",
    "      for j in range(i+1, len(boxes)):\n",
    "        # If they are of the same class\n",
    "        # and have a IoU above self.THRESHOLD\n",
    "        # we regard them as describing the same object and\n",
    "        # set the confidence of the box with lower confidence to 0\n",
    "        < your code here >\n",
    "\n",
    "    idxs = np.where(confidences>0)\n",
    "    self.boxes = boxes[idxs]\n",
    "    self.confidences = confidences[idxs]\n",
    "    self.classIDs = classIDs[idxs]\n",
    "  \n",
    "  def detect(self, image):\n",
    "    \"\"\"\n",
    "    detect objects, supress non maximums, draw boxes\n",
    "    return image with boxes\n",
    "    \"\"\"\n",
    "    self.forward(image)\n",
    "    self.non_max_supression()\n",
    "    \n",
    "    # draw boxes\n",
    "    for i in range(len(self.boxes)):\n",
    "      # extract the bounding box coordinates\n",
    "      (x, y) = (self.boxes[i][0], self.boxes[i][1])\n",
    "      (w, h) = (self.boxes[i][2], self.boxes[i][3])\n",
    "\n",
    "      # draw a bounding box rectangle and label on the image\n",
    "      color = self.COLORS[self.classIDs[i]].tolist()\n",
    "      cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "      text = \"{}: {:.4f}\".format(yolo.LABELS[self.classIDs[i]], self.confidences[i])\n",
    "      cv2.putText(image, text, (x, y + h - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    return image\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "VtervF6qCvc-",
    "outputId": "344b563b-6308-49c4-9bc9-514361ce67db"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-526cd6e0c609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPATH_TO_IMAGE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'yolo_data/yolo-object-detection/images/baggage_claim.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0myolo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_TO_YOLO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIDENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTHRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_TO_IMAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'YOLO' is not defined"
     ]
    }
   ],
   "source": [
    "PATH_TO_IMAGE = 'yolo_data/yolo-object-detection/images/baggage_claim.jpg'\n",
    "\n",
    "yolo = YOLO(PATH_TO_YOLO, CONFIDENCE, THRESHOLD)  \n",
    "image = cv2.imread(PATH_TO_IMAGE)\n",
    "image = yolo.detect(image)\n",
    "cv2_imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "8aYRMsadMBwW",
    "outputId": "6dd2988d-9e6c-498b-f53b-8a3e25e8654b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-10 07:18:35--  https://drive.google.com/file/d/1Xcq9FH2FdgZH1EQLK0kaQxntqH5bPWny/view?usp=sharing\n",
      "Resolving drive.google.com (drive.google.com)... 74.125.31.100, 74.125.31.101, 74.125.31.102, ...\n",
      "Connecting to drive.google.com (drive.google.com)|74.125.31.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘2012_000160.jpg’\n",
      "\n",
      "2012_000160.jpg         [ <=>                ]  69.08K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2020-09-10 07:18:35 (38.3 MB/s) - ‘2012_000160.jpg’ saved [70739]\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-46f20ab6b738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#image = cv2.imread('pascal/VOCdevkit/VOC2012/JPEGImages/2012_000160.jpg')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wget 'https://drive.google.com/file/d/1Xcq9FH2FdgZH1EQLK0kaQxntqH5bPWny/view?usp=sharing' -O '2012_000160.jpg'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2012_000160.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# print(os.listdir('/content/pascal/VOCdevkit/VOC2012/JPEGImages'))\n",
    "#image = cv2.imread('pascal/VOCdevkit/VOC2012/JPEGImages/2012_000160.jpg')\n",
    "image = cv2.imread('2012_000160.jpg')\n",
    "image = yolo.detect(image)\n",
    "cv2_imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOtoIXbQfGqT"
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('traffic.jpeg')\n",
    "image = yolo.detect(image)\n",
    "cv2_imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZjzLalstlwPk"
   },
   "source": [
    "## Mean Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQ2BQhu_lyZn"
   },
   "outputs": [],
   "source": [
    "def average_precision(precision, recall):\n",
    "  AP, MAP = 0, 0\n",
    "  # transform precision\n",
    "  unique_recall = np.unique(recall)\n",
    "  for i in range(1, len(unique_recall)):\n",
    "    r = unique_recall[i]\n",
    "    idx = np.where(recall == r)\n",
    "    AP += (r-unique_recall[i-1]) * np.max(precision[idx])\n",
    "  \n",
    "  return AP\n",
    "\n",
    "def mean_average_precision(pred_boxes, ground_truth_boxes):\n",
    "  \"\"\"\n",
    "  \n",
    "  \"\"\"\n",
    "  pred_box, pred_class, pred_confidence = pred_boxes[0], pred_boxes[1], pred_boxes[2]\n",
    "  gt_box = ground_truth_boxes[0]\n",
    "  \n",
    "  # we will calculate AP for each class, hence we need\n",
    "  # to calculate precision and recall for each class\n",
    "  AP_class = {}\n",
    "  \n",
    "  # first rank predictions by their confidence\n",
    "  idxs = np.argsort(pred_confidence)\n",
    "  pred_box, pred_class, pred_confidence = pred_box[idxs], pred_class[idxs], pred_confidence[idxs]\n",
    "  gt_box = gt_box[idxs]\n",
    "  \n",
    "  unique_classes = np.unique(pred_class)\n",
    "  \n",
    "  # calculate AP for each class\n",
    "  for c in unique_classes:\n",
    "    # take data for that class only\n",
    "    idxs = np.where(pred_class == c)\n",
    "    pb_boxes = pred_box[idxs]\n",
    "    gt_boxes = gt_box[idxs]\n",
    "    \n",
    "    TP = FP = 0\n",
    "    Precision = []\n",
    "    Recall = []\n",
    "    \n",
    "    for pb, gt in zip(pb_boxes, gt_boxes):\n",
    "      x,y,w,h = pred_box[i][0],  pred_box[i][1], pred_box[i][2], pred_box[i][3]\n",
    "      box1 = [x-w/2, y+h/2, x+w/2, y-h/2]\n",
    "      x,y,w,h = gt_box[i][0],  gt_box[i][1], gt_box[i][2], gt_box[i][3]\n",
    "      box2 = [x-w/2, y+h/2, x+w/2, y-h/2]\n",
    "      \n",
    "      if IOU(box1, box2) > 0.5:\n",
    "          TP =TP+1\n",
    "      else:\n",
    "          FP =FP+1    \n",
    "      try:\n",
    "          Pre = TP/(TP+FP)\n",
    "          Rec = TP/len(idxs)\n",
    "      except ZeroDivisionError:\n",
    "          Pre = Rec = 0.0\n",
    "      \n",
    "      Precision.append(Pre)\n",
    "      Recall.append(Rec)\n",
    "      \n",
    "    # now calculate the AP for that class\n",
    "    AP = average_precision(Precision, Recall)\n",
    "    AP_class[c] = AP\n",
    "    MAP += AP\n",
    "\n",
    "  # now after we computed AP for each class we can calculate MAP \n",
    "  return MAP/len(unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nk8JYHoPrXLC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0INW_IR9_pX8"
   },
   "source": [
    "## Comparing Detection and Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "id": "aT3VLcdGlw4V",
    "outputId": "cfa0dac1-903c-46be-c81d-079138e4139f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8bd1f51182d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#image = cv2.imread('pascal/VOCdevkit/VOC2012/JPEGImages/2012_000160.jpg')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2012_000160.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'yolo' is not defined"
     ]
    }
   ],
   "source": [
    "#image = cv2.imread('pascal/VOCdevkit/VOC2012/JPEGImages/2012_000160.jpg')\n",
    "image = cv2.imread('2012_000160.jpg')\n",
    "image = yolo.detect(image)\n",
    "cv2_imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SmPg6K9Nv3r"
   },
   "outputs": [],
   "source": [
    "#os.listdir('/content/pascal/VOCdevkit/VOC2012/Annotations')\n",
    "#mydoc = minidom.parse('pascal/VOCdevkit/VOC2012/Annotations/2012_000160.xml')\n",
    "mydoc = minidom.parse('2012_000160.xml')\n",
    "# pretty_xml_as_string = mydoc.toprettyxml()\n",
    "# print(pretty_xml_as_string)\n",
    "items = mydoc.getElementsByTagName('object')\n",
    "for elem in items:\n",
    "    print(elem.getElementsByTagName('xmax')[0].firstChild.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lMZdMBejAV4W"
   },
   "source": [
    "## References\n",
    "\n",
    "Original YOLO paper [link](https://arxiv.org/pdf/1506.02640.pdf)\n",
    "\n",
    "YOLOv2 paper [link](https://arxiv.org/pdf/1612.08242v1.pdf)\n",
    "\n",
    "YOLO explained [link](https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088)\n",
    "\n",
    "IoU [link](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)\n",
    "\n",
    "Non-Maximum Supression [link](https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/)\n",
    "\n",
    "Overview of deep learning based object detection approaches [link](https://towardsdatascience.com/beginners-guide-to-object-detection-algorithms-6620fb31c375)\n",
    "\n",
    "Mean Average Precision [link](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab8V2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
